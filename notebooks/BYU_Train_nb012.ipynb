{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "19c3e5ff",
      "metadata": {
        "id": "19c3e5ff"
      },
      "source": [
        "# Prepare Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9b71857f",
      "metadata": {
        "id": "9b71857f"
      },
      "outputs": [],
      "source": [
        "# Library\n",
        "\n",
        "import platform\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.amp import autocast, GradScaler\n",
        "import torchvision\n",
        "from glob import glob\n",
        "import cv2\n",
        "import warnings\n",
        "\n",
        "from scipy.signal import argrelmax\n",
        "\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# import albumentations as A\n",
        "from sklearn.model_selection import train_test_split\n",
        "# import segmentation_models_pytorch as smp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "8226870b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8226870b",
        "outputId": "b4dcb1ef-4f3d-440f-9657-a5e5c85943ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on a Mac\n",
            "GPU not available, using CPU\n"
          ]
        }
      ],
      "source": [
        "# Find session type\n",
        "def find_session_type():\n",
        "    # windows\n",
        "    if os.name == 'nt':\n",
        "        path = '../input/data/'\n",
        "\n",
        "        import japanize_matplotlib\n",
        "        sns.set(font=\"IPAexGothic\")\n",
        "\n",
        "    elif platform.system()  == 'Darwin':\n",
        "        # Mac\n",
        "        path = '../input/data/'\n",
        "        return 'mac'\n",
        "\n",
        "    elif os.name == 'posix':\n",
        "    # Kaggle\n",
        "        if 'KAGGLE_DATA_PROXY_TOKEN' in os.environ.keys():\n",
        "            print('This is kaggle session')\n",
        "            return 'kaggle'\n",
        "\n",
        "    # Google Colab\n",
        "        else:\n",
        "            print('This is colab session')\n",
        "            # セッションの残り時間の確認\n",
        "            !cat /proc/uptime | awk '{print $1 /60 /60 /24 \"days (\" $1 / 60 / 60 \"h)\"}'\n",
        "            return 'colab'\n",
        "    # Mac\n",
        "    # elif 'MAC' in os.environ.keys():\n",
        "    #     print('This is mac session')\n",
        "    #     return 'mac'\n",
        "# Example usage:\n",
        "if find_session_type() == 'kaggle':\n",
        "    print(\"Running in a Kaggle notebook\")\n",
        "elif find_session_type() == 'mac':\n",
        "    print(\"Running on a Mac\")\n",
        "    path = '../input/data/'\n",
        "\n",
        "elif find_session_type() == 'colab':\n",
        "    from google.colab import drive\n",
        "    print(\"Running in Google Colab\")\n",
        "    drive.mount('/content/drive')\n",
        "    os.makedirs('/content/logs', exist_ok=True)\n",
        "    os.makedirs('/content/kaggle/input', exist_ok=True)\n",
        "    os.makedirs('/content/kaggle/output', exist_ok=True)\n",
        "else:\n",
        "    print(\"Not running in a Kaggle notebook or Google Colab\")\n",
        "\n",
        "# if is_google_colab():\n",
        "#   byu_locating_bacterial_flagellar_motors_2025_path = kagglehub.competition_download('byu-locating-bacterial-flagellar-motors-2025')\n",
        "\n",
        "#   print('Data source import complete.')\n",
        "\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.seed = 42\n",
        "        self.debug = False\n",
        "        self.epochs = 10\n",
        "        self.num_classes = 86\n",
        "        self.ramdom_seed = 42\n",
        "        self.batch_size = 4\n",
        "        self.no_motor_use = 10\n",
        "\n",
        "        self.train_size = 0.8\n",
        "\n",
        "        self.root_dir = ''\n",
        "        if find_session_type() == 'kaggle':\n",
        "            self.root_dir = '/kaggle/'\n",
        "        elif find_session_type() == 'colab':\n",
        "            self.root_dir = '/content/kaggle/'\n",
        "        elif find_session_type() == 'mac':\n",
        "            self.root_dir = '../'\n",
        "        self.working_dir = os.path.join(self.root_dir, 'working')\n",
        "        self.pred_dir = os.path.join(self.root_dir, 'input', 'byu-locating-bacterial-flagellar-motors-2025', 'test')\n",
        "        self.data_dir = os.path.join(self.root_dir, 'input', 'byu-locating-bacterial-flagellar-motors-2025')\n",
        "        self.output_dir = os.path.join(self.root_dir, 'output')\n",
        "        self.dataset_dir = os.path.join(self.working_dir, 'dataset')\n",
        "        self.train_image_dir = os.path.join(self.dataset_dir, 'images','train')\n",
        "        self.train_label_dir = os.path.join(self.dataset_dir, 'labels','train')\n",
        "        self.val_image_dir = os.path.join(self.dataset_dir, 'images','val')\n",
        "        self.val_label_dir = os.path.join(self.dataset_dir, 'labels','val')\n",
        "        self.pred_dataset_dir = os.path.join(self.dataset_dir, 'test')\n",
        "\n",
        "        os.makedirs(self.dataset_dir, exist_ok=True)\n",
        "        os.makedirs(self.train_image_dir, exist_ok=True)\n",
        "        os.makedirs(self.train_label_dir, exist_ok=True)\n",
        "        os.makedirs(self.val_image_dir, exist_ok=True)\n",
        "        os.makedirs(self.val_label_dir, exist_ok=True)\n",
        "\n",
        "        assert os.path.exists(self.data_dir), f\"Data directory {self.data_dir} does not exist.\"\n",
        "\n",
        "        # 1st stage\n",
        "        self.low_resol_setting = {\n",
        "            \"voxel_size\": 16,\n",
        "            \"image_path\": os.path.join(self.working_dir, 'low_resol', 'images'),\n",
        "            \"label_path\": os.path.join(self.working_dir, 'low_resol', 'labels'),\n",
        "            \"model_fol\": os.path.join(self.output_dir, 'low_resol', 'model'),\n",
        "            \"cov_val\" : 5e-4,\n",
        "            \"sharpness\": 0.5,\n",
        "            \"threshold\": 0.6\n",
        "        }\n",
        "\n",
        "        # 2nd stage\n",
        "        self.high_resol_setting = {\n",
        "            \"voxel_size\": 64,\n",
        "            \"translate\": 0.2,\n",
        "            \"image_path\": os.path.join(self.working_dir, 'high_resol'),\n",
        "            \"label_path\": os.path.join(self.working_dir, 'high_resol', 'labels'),\n",
        "            \"model_fol\": os.path.join(self.output_dir, 'high_resol', 'model'),\n",
        "            \"cov_val\" : 1e-4,\n",
        "            \"crop_thresh\": 0.5,\n",
        "            \"sharpness\": 0.5,\n",
        "            \"threshold\": 0.8\n",
        "            # positive: negative= 1:1,\n",
        "        }\n",
        "        os.makedirs(self.low_resol_setting['image_path'], exist_ok=True)\n",
        "        os.makedirs(self.low_resol_setting['label_path'], exist_ok=True)\n",
        "        os.makedirs(self.low_resol_setting['model_fol'], exist_ok=True)\n",
        "        os.makedirs(self.high_resol_setting['image_path'], exist_ok=True)\n",
        "        os.makedirs(self.high_resol_setting['label_path'], exist_ok=True)\n",
        "        os.makedirs(self.high_resol_setting['model_fol'], exist_ok=True)\n",
        "        # Torch settings\n",
        "        self.lr = 1e-4\n",
        "        self.grad_norm = 0.1\n",
        "        self.init_scale = 4096\n",
        "\n",
        "        random.seed(self.ramdom_seed)\n",
        "        np.random.seed(self.ramdom_seed)\n",
        "        torch.manual_seed(self.ramdom_seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed(self.ramdom_seed)\n",
        "            torch.backends.cudnn.deterministic = True\n",
        "\n",
        "class GPUProfiler:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.start_time = None\n",
        "\n",
        "    def __enter__(self):\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        self.start_time = time.time()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        elapsed = time.time() - self.start_time\n",
        "        # print(f\"[PROFILE] {self.name}: {elapsed:.3f}s\")\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "if device.startswith('cuda'):\n",
        "    # Set CUDA optimization flags\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 on Ampere GPUs\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    # Print GPU info\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9  # Convert to GB\n",
        "    print(f\"Using GPU: {gpu_name} with {gpu_mem:.2f} GB memory\")\n",
        "\n",
        "    # Get available GPU memory and set batch size accordingly\n",
        "    free_mem = gpu_mem - torch.cuda.memory_allocated(0) / 1e9\n",
        "    BATCH_SIZE = max(8, min(32, int(free_mem * 4)))  # 4 images per GB as rough estimate\n",
        "    print(f\"Dynamic batch size set to {BATCH_SIZE} based on {free_mem:.2f}GB free memory\")\n",
        "else:\n",
        "    print(\"GPU not available, using CPU\")\n",
        "    BATCH_SIZE = 4  # Reduce batch size for CPU\n",
        "\n",
        "config = Config()\n",
        "\n",
        "config.batch_size = BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1a65755",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1a65755",
        "outputId": "397b85b3-e65c-4cd9-e5f0-62e03b7aba59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is colab session\n",
            "0.00907975days (0.217914h)\n",
            "This is colab session\n",
            "0.0090809days (0.217942h)\n"
          ]
        }
      ],
      "source": [
        "if find_session_type() == 'kaggle':\n",
        "    config.data_dir = '/kaggle/input/byu-locating-bacterial-flagellar-motors-2025'\n",
        "    config.output_dir = '/kaggle/output'\n",
        "    config.dataset_dir = '/kaggle/working/dataset'\n",
        "elif find_session_type() == 'colab':\n",
        "    config.data_dir = '/content/kaggle/input/byu-locating-bacterial-flagellar-motors-2025'\n",
        "    config.output_dir = '/content/kaggle/output'\n",
        "    config.dataset_dir = os.path.join(config.working_dir, 'dataset')\n",
        "    os.makedirs(config.dataset_dir, exist_ok=True)\n",
        "    datapath = '/content/drive/MyDrive/kaggle/BYU-motor-detection/datas/3d-low-resol'\n",
        "    !unzip -q {datapath} -d {config.dataset_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9f5ec00",
      "metadata": {
        "id": "a9f5ec00"
      },
      "source": [
        "# Helper Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54153bcb",
      "metadata": {
        "id": "54153bcb"
      },
      "source": [
        "3D-CNNに読み込ませる\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ce28456",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ce28456",
        "outputId": "d4040293-b6d5-409d-b1aa-f3be686fdc8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UNet3D(\n",
            "  (enc1): DoubleConv(\n",
            "    (double_conv): Sequential(\n",
            "      (0): Conv3d(1, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
            "      (1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv3d(8, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
            "      (4): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (enc2): DoubleConv(\n",
            "    (double_conv): Sequential(\n",
            "      (0): Conv3d(8, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
            "      (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
            "      (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (enc3): DoubleConv(\n",
            "    (double_conv): Sequential(\n",
            "      (0): Conv3d(32, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
            "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
            "      (4): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (enc4): DoubleConv(\n",
            "    (double_conv): Sequential(\n",
            "      (0): Conv3d(128, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
            "      (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
            "      (4): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (bottleneck): DoubleConv(\n",
            "    (double_conv): Sequential(\n",
            "      (0): Conv3d(512, 1024, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
            "      (1): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv3d(1024, 1024, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
            "      (4): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (up4): UpConv_3D(\n",
            "    (up): Upsample(scale_factor=2.0, mode='trilinear')\n",
            "    (bn1): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv): Conv3d(1024, 512, kernel_size=(2, 2, 2), stride=(1, 1, 1), padding=same)\n",
            "    (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (dec4): DoubleConv(\n",
            "    (double_conv): Sequential(\n",
            "      (0): Conv3d(1024, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
            "      (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
            "      (4): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (up3): UpConv_3D(\n",
            "    (up): Upsample(scale_factor=2.0, mode='trilinear')\n",
            "    (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv): Conv3d(512, 128, kernel_size=(2, 2, 2), stride=(1, 1, 1), padding=same)\n",
            "    (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (dec3): DoubleConv(\n",
            "    (double_conv): Sequential(\n",
            "      (0): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
            "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
            "      (4): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (up2): UpConv_3D(\n",
            "    (up): Upsample(scale_factor=2.0, mode='trilinear')\n",
            "    (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv): Conv3d(128, 32, kernel_size=(2, 2, 2), stride=(1, 1, 1), padding=same)\n",
            "    (bn2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (dec2): DoubleConv(\n",
            "    (double_conv): Sequential(\n",
            "      (0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
            "      (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
            "      (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (up1): UpConv_3D(\n",
            "    (up): Upsample(scale_factor=2.0, mode='trilinear')\n",
            "    (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv): Conv3d(32, 8, kernel_size=(2, 2, 2), stride=(1, 1, 1), padding=same)\n",
            "    (bn2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (dec1): DoubleConv(\n",
            "    (double_conv): Sequential(\n",
            "      (0): Conv3d(16, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
            "      (1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv3d(8, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
            "      (4): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (conv1): Conv3d(8, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
            "  (pool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Model developing\n",
        "\n",
        "# 3D U-net by pytorch\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_ch, mdl_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv3d(in_ch, out_ch, kernel_size=3, padding=\"same\"),\n",
        "            nn.BatchNorm3d(mdl_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(mdl_ch, out_ch, kernel_size=3, padding=\"same\"),\n",
        "            nn.BatchNorm3d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "class UpConv_3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.Upsample(scale_factor=2, mode=\"trilinear\", align_corners=True)\n",
        "        self.bn1 = nn.BatchNorm3d(in_channels)\n",
        "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size = 2, padding=\"same\")\n",
        "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.bn2(x)\n",
        "        return x\n",
        "\n",
        "class UNet3D(nn.Module):\n",
        "    def __init__(self, mode):\n",
        "        super().__init__()\n",
        "        if mode == 'low':\n",
        "            self.enc1 = DoubleConv(1, 8, 8)\n",
        "            self.enc2 = DoubleConv(8, 32, 32)\n",
        "            self.enc3 = DoubleConv(32, 128, 128)\n",
        "            self.enc4 = DoubleConv(128, 512, 512)\n",
        "\n",
        "            self.bottleneck = DoubleConv(512, 1024, 1024)\n",
        "\n",
        "            self.up4 = UpConv_3D(1024, 512)\n",
        "            self.dec4 = DoubleConv(1024, 512, 512)\n",
        "            self.up3 = UpConv_3D(512, 128)\n",
        "            self.dec3 = DoubleConv(256, 128, 128)\n",
        "            self.up2 = UpConv_3D(128, 32)\n",
        "            self.dec2 = DoubleConv(64, 32, 32)\n",
        "            self.up1 = UpConv_3D(32, 8)\n",
        "            self.dec1 = DoubleConv(16, 8, 8)\n",
        "\n",
        "            self.conv1 = nn.Conv3d(8, 1, kernel_size=1)\n",
        "\n",
        "            self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "\n",
        "        if mode == 'high':\n",
        "            self.enc1 = DoubleConv(1, 16, 16)\n",
        "            self.enc2 = DoubleConv(16, 64, 64)\n",
        "            self.enc3 = DoubleConv(64, 128, 128)\n",
        "            self.enc4 = DoubleConv(128, 256, 256)\n",
        "            self.enc5 = DoubleConv(256, 512, 512)\n",
        "\n",
        "            self.bottleneck = DoubleConv(512, 1024, 1024)\n",
        "\n",
        "            self.up5 = UpConv_3D(1024, 512)\n",
        "            self.dec5 = DoubleConv(1024, 512, 512)\n",
        "            self.up4 = UpConv_3D(512, 256)\n",
        "            self.dec4 = DoubleConv(512, 256, 256)\n",
        "            self.up3 = UpConv_3D(256, 128)\n",
        "            self.dec3 = DoubleConv(256, 128, 128)\n",
        "            self.up2 = UpConv_3D(128, 64)\n",
        "            self.dec2 = DoubleConv(128, 64, 64)\n",
        "            self.up1 = UpConv_3D(64, 16)\n",
        "            self.dec1 = DoubleConv(32, 16, 16)\n",
        "\n",
        "            self.conv1 = nn.Conv3d(16, 1, kernel_size=1)\n",
        "\n",
        "            self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.enc1(x)\n",
        "        x1 = x\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.enc2(x)\n",
        "        x2 = x\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.enc3(x)\n",
        "        x3 = x\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.enc4(x)\n",
        "        x4 = x\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        x = self.up4(x)\n",
        "        x = torch.cat((x, x4), dim=1)\n",
        "        x = self.dec4(x)\n",
        "\n",
        "        x = self.up3(x)\n",
        "        x = torch.cat((x, x3), dim=1)\n",
        "        x = self.dec3(x)\n",
        "\n",
        "        x = self.up2(x)\n",
        "        x = torch.cat((x, x2), dim=1)\n",
        "        x = self.dec2(x)\n",
        "\n",
        "        x = self.up1(x)\n",
        "        x = torch.cat((x, x1), dim=1)\n",
        "        x = self.dec1(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        return x\n",
        "\n",
        "def check_dataset_consistency(dataset_or_loader, num_samples=5, visualize=True):\n",
        "    \"\"\"\n",
        "    3D-UNet用データセットまたはデータローダーのバリデーションを行う関数。\n",
        "\n",
        "    Args:\n",
        "        dataset_or_loader: PyTorch DatasetまたはDataLoaderインスタンス\n",
        "        num_samples: チェックするサンプル数\n",
        "        visualize: Trueなら入力・ラベルを可視化する\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Dataset Consistency Check Start ===\\n\")\n",
        "\n",
        "    if hasattr(dataset_or_loader, 'dataset'):\n",
        "        # DataLoaderの場合\n",
        "        dataset = dataset_or_loader.dataset\n",
        "        loader = dataset_or_loader\n",
        "        use_loader = True\n",
        "    else:\n",
        "        # Datasetの場合\n",
        "        dataset = dataset_or_loader\n",
        "        use_loader = False\n",
        "\n",
        "    if use_loader:\n",
        "        data_iter = iter(loader)\n",
        "    else:\n",
        "        indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        if use_loader:\n",
        "            batch = next(data_iter)\n",
        "            X, Y = batch\n",
        "            X, Y = X[0], Y[0]  # バッチの最初のサンプルを使用\n",
        "        else:\n",
        "            idx = indices[i]\n",
        "            X, Y = dataset[idx]\n",
        "\n",
        "        # --- 1. 形状チェック ---\n",
        "        assert X.ndim == 4, f\"Input X should have 4 dimensions (C,D,H,W), got {X.shape}\"\n",
        "        assert Y.ndim == 3 or Y.ndim == 4, f\"Label Y should have 3 or 4 dimensions, got {Y.shape}\"\n",
        "\n",
        "        # --- 2. チャンネル数確認 ---\n",
        "        C, D, H, W = X.shape\n",
        "        assert C in [1, 3], f\"Unexpected channel count {C}\"\n",
        "\n",
        "        # --- 3. 値の範囲確認 ---\n",
        "        if torch.is_tensor(X):\n",
        "            X_min, X_max = X.min().item(), X.max().item()\n",
        "        else:\n",
        "            X_min, X_max = np.min(X), np.max(X)\n",
        "\n",
        "        print(f\"Sample {i}: Input value range = ({X_min:.3f}, {X_max:.3f})\")\n",
        "        assert not np.isnan(X_min) and not np.isnan(X_max), \"NaN detected in input\"\n",
        "\n",
        "        # --- 4. 型確認 ---\n",
        "        assert isinstance(X, (torch.Tensor, np.ndarray)), \"Input X must be Tensor or ndarray\"\n",
        "        assert isinstance(Y, (torch.Tensor, np.ndarray)), \"Label Y must be Tensor or ndarray\"\n",
        "\n",
        "        # --- 5. 欠損値チェック ---\n",
        "        if torch.isnan(X).any() or torch.isnan(Y).any():\n",
        "            raise ValueError(f\"NaN found in sample {i}\")\n",
        "\n",
        "        # --- 6. 入力とラベルの対応確認 ---\n",
        "        if Y.ndim == 4:\n",
        "            assert Y.shape[1:] == X.shape[1:], f\"Mismatch between input and label shape: {X.shape} vs {Y.shape}\"\n",
        "        else:\n",
        "            assert Y.shape == X.shape[1:], f\"Mismatch between input and label shape: {X.shape} vs {Y.shape}\"\n",
        "\n",
        "        # --- 7. 簡易可視化 ---\n",
        "        if visualize:\n",
        "            slice_idx = D // 2\n",
        "            fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
        "\n",
        "            axs[0].imshow(X[0, slice_idx].cpu().numpy(), cmap='gray')\n",
        "            axs[0].set_title('Input (Mid slice)')\n",
        "\n",
        "            if Y.ndim == 4:\n",
        "                axs[1].imshow(Y[0, slice_idx].cpu().numpy(), cmap='gray')\n",
        "            else:\n",
        "                axs[1].imshow(Y[slice_idx].cpu().numpy(), cmap='gray')\n",
        "            axs[1].set_title('Label (Mid slice)')\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "    print(\"\\n=== Dataset Consistency Check Completed ===\\n\")\n",
        "# Check the model network\n",
        "model = UNet3D(mode='low').to(device)\n",
        "print(model)\n",
        "\n",
        "# Loss function\n",
        "def criterion(pred, target):\n",
        "    \"\"\"\n",
        "    Custom loss function for 3D U-Net model.\n",
        "    Args:\n",
        "        pred (torch.Tensor): Predicted output from the model.\n",
        "        target (torch.Tensor): Ground truth labels.\n",
        "    Returns:\n",
        "        torch.Tensor: Computed loss value.\n",
        "    \"\"\"\n",
        "    loss_fn = nn.SmoothL1Loss(reduction='mean')\n",
        "    # Calculate the loss using the defined loss function\n",
        "    loss = loss_fn(pred, target)\n",
        "    return loss\n",
        "\n",
        "def three_d_nms(pb_tensor:np.ndarray, threshold=0.5, kernel_size=3):\n",
        "\n",
        "    \"\"\"\n",
        "    Apply 3D Non-Maximum Suppression (NMS) to the input tensor.\n",
        "    Args:\n",
        "        pb_tensor (np.ndarray): Posibility Distrubution Function map\n",
        "    \"\"\"\n",
        "\n",
        "    assert pb_tensor.ndim == 3, f\"Input tensor must have 4 dimensions (D, H, W)\"\n",
        "    return pb_tensor\n",
        "    \n",
        "def predict_on_samples(model:torch.nn.Module, dataloader:DataLoader, settings:dict, num_samples=5):\n",
        "\n",
        "    \"\"\"\n",
        "    Predict on a few samples from the dataloader and visualize the results.\n",
        "    Args:\n",
        "        model (torch.nn.Module): The trained model.\n",
        "        dataloader (torch.utils.data.DataLoader): DataLoader for the dataset.\n",
        "        num_samples (int): Number of samples to visualize.\n",
        "        thresh (float): Threshold for binarizing the predictions.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    fig, axs = plt.subplots(3, num_samples, figsize=(12, 4))\n",
        "    with torch.no_grad():\n",
        "        for i, (X, Y) in tqdm(enumerate(dataloader)):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "            X = X.to(device)\n",
        "            Y = Y.to(device)\n",
        "\n",
        "            pred = model(X)\n",
        "            pred = torch.sigmoid(pred)\n",
        "            pred = pred.to(device).numpy()\n",
        "\n",
        "            pos = (pred > settings['threshold']) * pred\n",
        "\n",
        "            # Visualize the first sample\n",
        "            fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "            slice_idx = X.shape[2] // 2\n",
        "\n",
        "            axs[i,0].imshow(X[0, 0, slice_idx].cpu().numpy(), cmap='gray')\n",
        "            axs[i,0].set_title('Input')\n",
        "\n",
        "            axs[i,1].imshow(Y[0, slice_idx].cpu().numpy(), cmap='hot')\n",
        "            axs[i,1].set_title('Ground Truth')\n",
        "\n",
        "            axs[i,2].imshow(pred[0, 0, slice_idx].cpu().numpy(), cmap='hot')\n",
        "            axs[i,2].set_title('Prediction')\n",
        "\n",
        "        fig.show()\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, label_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = glob(os.path.join(image_dir, '*.npy'))\n",
        "        self.label_files = glob(os.path.join(label_dir, '*.npy'))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_files[idx]\n",
        "        label_path = self.label_files[idx]\n",
        "\n",
        "        try:\n",
        "            image_np = np.load(image_path, mmap_mode='r').copy()\n",
        "            label_np = np.load(label_path, mmap_mode='r').copy()\n",
        "\n",
        "            image_tensor = torch.from_numpy(image_np).float()\n",
        "            label_tensor = torch.from_numpy(label_np).float()\n",
        "\n",
        "            if image_tensor.ndim == 3:\n",
        "                image_tensor = image_tensor.unsqueeze(0)\n",
        "\n",
        "            if self.transform:\n",
        "                image_tensor = self.transform(image_tensor)\n",
        "\n",
        "            return image_tensor, label_tensor\n",
        "\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"読み込み失敗: {image_path}, {label_path}\\nエラー内容: {e}\")\n",
        "\n",
        "            # ダミーデータを返す（学習を止めずに続ける）\n",
        "            dummy_image = torch.zeros((1, 32, 32, 32), dtype=torch.float32)  # 例\n",
        "            dummy_label = torch.zeros((32, 32, 32), dtype=torch.float32)  # 例\n",
        "\n",
        "            return dummy_image, dummy_label\n",
        "\n",
        "def simple_3d_nms(pred, threshold=0.5, kernel_size=3):\n",
        "    \"\"\"\n",
        "    Simple 3D Non-Maximum Suppression (NMS) function.\n",
        "    Args:\n",
        "        pred (torch.Tensor): Input tensor of shape (C, D, H, W).\n",
        "        threshold (float): Threshold for NMS.\n",
        "        kernel_size (int): Size of the kernel for NMS.\n",
        "    Returns:\n",
        "        torch.Tensor: Output tensor after applying NMS.\n",
        "    \"\"\"\n",
        "\n",
        "    if pred.ndim == 4:\n",
        "        pred = pred.squeeze(0)\n",
        "    if pred.ndim != 4:\n",
        "        raise ValueError(\"Input tensor must have 4 dimensions (C, D, H, W)\")\n",
        "    pred = F.threshold(pred, threshold, 0)\n",
        "    pooled = F.max_pool3d(pred.unsqueeze(0).unsqueeze(0),\n",
        "                          kernel_size=kernel_size, stride=1,\n",
        "                          padding=kernel_size//2)\n",
        "\n",
        "    peaks = (pred == pooled) & (pooled > threshold)\n",
        "    peak_coords = torch.nonzero(peaks, as_tuple=False)\n",
        "    return peak_coords.cpu().numpy()\n",
        "\n",
        "def calculate_metrics(pred, target, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Calculate precision, recall, and F1 score for the predicted and target tensors.\n",
        "    Args:\n",
        "        pred (torch.Tensor): Predicted tensor.\n",
        "        target (torch.Tensor): Target tensor.\n",
        "        threshold (float): Threshold for binarizing the predictions.\n",
        "    Returns:\n",
        "        tuple: Precision, recall, and F1 score.\n",
        "    \"\"\"\n",
        "    pred = pred.float()\n",
        "    target = target.float()\n",
        "\n",
        "    mse = F.mse_loss(pred, target, reduction='mean').item()\n",
        "    mae = F.l1_loss(pred, target, reduction='mean').item()\n",
        "\n",
        "    pred_binary = (pred > threshold).float()\n",
        "    target_binary = (target > threshold).float()\n",
        "\n",
        "    # Dice the metrics\n",
        "    intersection = torch.sum(pred_binary * target_binary)\n",
        "    union = torch.sum(pred_binary + target_binary)\n",
        "\n",
        "    if union == 0:\n",
        "        dice = 1.0\n",
        "    else:\n",
        "        dice = (2 * intersection / union).item()\n",
        "\n",
        "    iou = (intersection / (torch.sum(pred_binary) + torch.sum(target_binary) - intersection+1e-7)).item()\n",
        "\n",
        "    return {\n",
        "        'mse': round(mse, 5),\n",
        "        'mae': round(mae, 5),\n",
        "        'dice': round(dice, 5),\n",
        "        'iou' : round(iou, 5)\n",
        "    }\n",
        "\n",
        "def find_motor_pos(voxel:np.ndarray, voxel_origin:tuple):\n",
        "    \"\"\"\n",
        "    Find the position of the motor in the voxel.\n",
        "    Args:\n",
        "        voxel (np.ndarray): The input voxel.\n",
        "        voxel_ab_size (tuple): The size of the voxel.\n",
        "        voxel_origin (tuple): The origin of the voxel.\n",
        "    Returns:\n",
        "        tuple: The position of the motor in the voxel.\n",
        "    \"\"\"\n",
        "    # Calculate the center of the motor\n",
        "    relative_pos = np.unravel_index(np.argmax(voxel), voxel.shape)\n",
        "\n",
        "    absolute_pos = np.array(relative_pos) + np.array(voxel_origin)\n",
        "    \n",
        "    return absolute_pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4f4cbdf3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(array([1, 2, 2, 3, 4]), array([0, 1, 4, 2, 5]))\n"
          ]
        }
      ],
      "source": [
        "pred = np.array([\n",
        "        [1, 4, 0, 9, 0, 3, 2],\n",
        "        [4, 0, 0, 1, 2, 3, 7],\n",
        "        [2, 9, 2, 0, 9, 0, 7],\n",
        "        [0, 0, 7, 9, 6, 3, 1],\n",
        "        [0, 4, 4, 7, 2, 8, 3]\n",
        "    ])\n",
        "pred = np.array(pred)\n",
        "pred_pos = argrelmax(pred, order=3, mode='wrap')\n",
        "print(pred_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8a1df815",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a1df815",
        "outputId": "2af9e1c7-ba8b-4bae-f8fd-2a70ca4ccd88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics: {'mse': 0.0925, 'mae': 0.175, 'dice': 1.0, 'iou': 1.0}\n"
          ]
        }
      ],
      "source": [
        "def test_metrics():\n",
        "    \"\"\"\n",
        "    Test the calculate_metrics function with dummy data.\n",
        "    \"\"\"\n",
        "    pred = torch.tensor([[[[0.1, 0.2], [0.3, 0.4]]]])\n",
        "    target = torch.tensor([[[[0.1, 0.1], [0.3, 1.0]]]])\n",
        "\n",
        "    metrics = calculate_metrics(pred, target, threshold=0.01)\n",
        "    print(\"Metrics:\", metrics)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_metrics()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2316b2e8",
      "metadata": {
        "id": "2316b2e8"
      },
      "source": [
        "### Train Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a57f9bac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a57f9bac",
        "outputId": "502e4365-608e-4f4c-e3b6-8e40f7a32bb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 297\n",
            "\n",
            "=== Dataset Consistency Check Start ===\n",
            "\n",
            "Sample 0: Input value range = (34.312, 247.531)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Dataset Consistency Check Completed ===\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]\n",
            "  0%|          | 0/75 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|▏         | 1/75 [00:30<37:08, 30.11s/it]\u001b[A\n",
            "  3%|▎         | 2/75 [00:58<35:42, 29.35s/it]\u001b[A\n",
            "  4%|▍         | 3/75 [01:28<35:14, 29.37s/it]\u001b[A\n",
            "  5%|▌         | 4/75 [01:58<34:56, 29.53s/it]\n",
            "  0%|          | 0/10 [01:58<?, ?it/s]\n"
          ]
        },
        {
          "ename": "UnpicklingError",
          "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6c19d629ae63>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-5bce17a359f6>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimage_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mimage_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mlabel_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1468\u001b[0m                         )\n\u001b[1;32m   1469\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1470\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1471\u001b[0m                 return _load(\n\u001b[1;32m   1472\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
          ]
        }
      ],
      "source": [
        "history = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': []\n",
        "}\n",
        "best_val_loss = float('inf')\n",
        "best_model_path = os.path.join(config.output_dir, 'best_model.pth')\n",
        "config.train_image_dir = os.path.join(config.dataset_dir, 'low_resol', 'images','train')\n",
        "config.train_label_dir = os.path.join(config.dataset_dir, 'low_resol', 'labels','train')\n",
        "config.val_image_dir = os.path.join(config.dataset_dir, 'low_resol', 'images','val')\n",
        "config.val_label_dir = os.path.join(config.dataset_dir, 'low_resol', 'labels','val')\n",
        "\n",
        "assert os.path.exists(config.train_image_dir), f\"Train image directory not found: {config.train_image_dir}\"\n",
        "assert os.path.exists(config.train_label_dir), f\"Train label directory not found: {config.train_label_dir}\"\n",
        "# Low resolution\n",
        "model = UNet3D(mode='low').to(device)\n",
        "setting = config.low_resol_setting\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
        "\n",
        "train_dataset = CustomDataset(config.train_image_dir, config.train_label_dir)\n",
        "val_dataset = CustomDataset(config.val_image_dir, config.val_label_dir)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "    shuffle=True)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "    shuffle=False)\n",
        "\n",
        "# Validation\n",
        "check_dataset_consistency(train_loader, num_samples=1, visualize=True)\n",
        "\n",
        "scaler = GradScaler(init_scale=config.init_scale)\n",
        "\n",
        "for epoch in tqdm(range(config.epochs)):\n",
        "    train_loss = 0.0\n",
        "    val_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast(device, dtype=torch.float16):\n",
        "            y = model(x)\n",
        "            loss = criterion(x, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        history['train_loss'].append(loss.item())\n",
        "        if config.debug:\n",
        "            break\n",
        "\n",
        "    #Validation loop\n",
        "    val_loss = 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for val_x, val_y in val_loader:\n",
        "            x_val = x_val.to(device).float() # Tensor\n",
        "            y_val = y_val.to(device).float() # Tensor\n",
        "\n",
        "            with autocast(device_type=device.type, dtype=torch.float16):\n",
        "                output = model(x_val)\n",
        "                loss = criterion(output, y_val)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            \n",
        "            # 予測と正解を蓄積\n",
        "            preds = output.sigmoid()  # 2値分類ならsigmoid\n",
        "            preds = (preds > setting['threshold']).float()\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_targets.append(y_val.cpu())\n",
        "    \n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    history['train_loss'].append(avg_train_loss)\n",
        "    history['val_loss'].append(avg_val_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{config.epochs}] Train Loss: {train_loss:.4f} Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "predict_on_samples(model, val_loader, num_samples=5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "byu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
