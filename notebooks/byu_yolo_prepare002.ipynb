{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f11e5757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "import yaml\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, fbeta_score\n",
    "import cv2\n",
    "import threading\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f66cafb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting\n",
    "DEBUG_MODE = False\n",
    "PATCH_SIZE = 640\n",
    "TRUST = 5\n",
    "BOX_SIZE = 30\n",
    "TRAIN_SPLIT = 0.8\n",
    "CONCENTRATION = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a620a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_slice(slice_data):\n",
    "    \"\"\"\n",
    "    Normalize slice data using the 2nd and 98th percentiles.\n",
    "    \n",
    "    Args:\n",
    "        slice_data (numpy.array): Input image slice.\n",
    "    \n",
    "    Returns:\n",
    "        np.uint8: Normalized image in the range [0, 255].\n",
    "    \"\"\"\n",
    "    p2 = np.percentile(slice_data, 2)\n",
    "    p98 = np.percentile(slice_data, 98)\n",
    "    clipped_data = np.clip(slice_data, p2, p98)\n",
    "    normalized = 255 * (clipped_data - p2) / (p98 - p2)\n",
    "    return np.uint8(normalized)\n",
    "\n",
    "# Define the preprocessing function to extract slices, normalize, and generate YOLO annotations.\n",
    "def prepare_yolo_dataset(trust=TRUST, train_split=TRAIN_SPLIT, dir_info=None):\n",
    "    \"\"\"\n",
    "    Extract slices containing motors and save images with corresponding YOLO annotations.\n",
    "    \n",
    "    Steps:\n",
    "    - Load the motor labels.\n",
    "    - Perform a train/validation split by tomogram.\n",
    "    - For each motor, extract slices in a range (Â± trust parameter).\n",
    "    - Normalize each slice and save it.\n",
    "    - Generate YOLO format bounding box annotations with a fixed box size.\n",
    "    - Create a YAML configuration file for YOLO training.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A summary containing dataset statistics and file paths.\n",
    "    \"\"\"\n",
    "    # Load the labels CSV\n",
    "    labels_df = pd.read_csv(os.path.join(dir_info['data_path'], \"train_labels.csv\"))\n",
    "    \n",
    "    total_motors = labels_df['Number of motors'].sum()\n",
    "    print(f\"Total number of motors in the dataset: {total_motors}\")\n",
    "    \n",
    "    # Consider only tomograms with at least one motor\n",
    "    tomo_df = labels_df[labels_df['Number of motors'] > 0].copy()\n",
    "    unique_tomos = tomo_df['tomo_id'].unique()\n",
    "    print(f\"Found {len(unique_tomos)} unique tomograms with motors\")\n",
    "    \n",
    "    # Shuffle and split tomograms into train and validation sets\n",
    "    np.random.shuffle(unique_tomos)\n",
    "    split_idx = int(len(unique_tomos) * train_split)\n",
    "    train_tomos = unique_tomos[:split_idx]\n",
    "    val_tomos = unique_tomos[split_idx:]\n",
    "    print(f\"Split: {len(train_tomos)} tomograms for training, {len(val_tomos)} tomograms for validation\")\n",
    "    \n",
    "    # Helper function to process a list of tomograms\n",
    "    def process_tomogram_set(tomogram_ids, images_dir, labels_dir, set_name):\n",
    "        motor_counts = []\n",
    "        for tomo_id in tomogram_ids:\n",
    "            # Get motor annotations for the current tomogram\n",
    "            tomo_motors = labels_df[labels_df['tomo_id'] == tomo_id]\n",
    "            for _, motor in tomo_motors.iterrows():\n",
    "                if pd.isna(motor['Motor axis 0']):\n",
    "                    continue\n",
    "                motor_counts.append(\n",
    "                    (tomo_id, \n",
    "                     int(motor['Motor axis 0']), \n",
    "                     int(motor['Motor axis 1']), \n",
    "                     int(motor['Motor axis 2']),\n",
    "                     int(motor['Array shape (axis 0)']))\n",
    "                )\n",
    "        \n",
    "        print(f\"Will process approximately {len(motor_counts) * (2 * trust + 1)} slices for {set_name}\")\n",
    "        processed_slices = 0\n",
    "        \n",
    "        # Loop over each motor annotation\n",
    "        for tomo_id, z_center, y_center, x_center, z_max in tqdm(motor_counts, desc=f\"Processing {set_name} motors\"):\n",
    "            z_min = max(0, z_center - trust)\n",
    "            z_max_bound = min(z_max - 1, z_center + trust)\n",
    "            for z in range(z_min, z_max_bound + 1):\n",
    "                if z % 3 == 0:\n",
    "                    continue\n",
    "                # Create the slice filename and source path\n",
    "                slice_filename = f\"slice_{z:04d}.jpg\"\n",
    "                src_path = os.path.join(dir_info['train_dir'], tomo_id, slice_filename)\n",
    "                if not os.path.exists(src_path):\n",
    "                    print(f\"Warning: {src_path} does not exist, skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                # Load, normalize, and save the image slice\n",
    "                img = Image.open(src_path)\n",
    "                img_array = np.array(img)\n",
    "                normalized_img = normalize_slice(img_array)\n",
    "                dest_filename = f\"{tomo_id}_z{z:04d}_y{y_center:04d}_x{x_center:04d}.jpg\"\n",
    "                dest_path = os.path.join(images_dir, dest_filename)\n",
    "                Image.fromarray(normalized_img).save(dest_path)\n",
    "                \n",
    "                # Prepare YOLO bounding box annotation (normalized values)\n",
    "                img_width, img_height = img.size\n",
    "                x_center_norm = x_center / img_width\n",
    "                y_center_norm = y_center / img_height\n",
    "                box_width_norm = BOX_SIZE / img_width\n",
    "                box_height_norm = BOX_SIZE / img_height\n",
    "                label_path = os.path.join(labels_dir, dest_filename.replace('.jpg', '.txt'))\n",
    "                with open(label_path, 'w') as f:\n",
    "                    f.write(f\"0 {x_center_norm} {y_center_norm} {box_width_norm} {box_height_norm}\\n\")\n",
    "                \n",
    "                processed_slices += 1\n",
    "        \n",
    "        return processed_slices, len(motor_counts)\n",
    "    \n",
    "    # Process training tomograms\n",
    "    train_slices, train_motors = process_tomogram_set(train_tomos, dir_info['yolo_images_train'], dir_info['yolo_labels_train'], \"training\")\n",
    "    # Process validation tomograms\n",
    "    val_slices, val_motors = process_tomogram_set(val_tomos, dir_info['yolo_images_val'], dir_info['yolo_labels_val'], \"validation\")\n",
    "    \n",
    "    # Generate YAML configuration for YOLO training\n",
    "    yaml_content = {\n",
    "        'path': dir_info['yolo_dataset_dir'],\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'names': {0: 'motor'}\n",
    "    }\n",
    "    with open(os.path.join(dir_info['yolo_dataset_dir'], 'dataset.yaml'), 'w') as f:\n",
    "        yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"\\nProcessing Summary:\")\n",
    "    print(f\"- Train set: {len(train_tomos)} tomograms, {train_motors} motors, {train_slices} slices\")\n",
    "    print(f\"- Validation set: {len(val_tomos)} tomograms, {val_motors} motors, {val_slices} slices\")\n",
    "    print(f\"- Total: {len(train_tomos) + len(val_tomos)} tomograms, {train_motors + val_motors} motors, {train_slices + val_slices} slices\")\n",
    "    \n",
    "    return {\n",
    "        \"dataset_dir\": dir_info['yolo_dataset_dir'],\n",
    "        \"yaml_path\": os.path.join(dir_info['yolo_dataset_dir'], 'dataset.yaml'),\n",
    "        \"train_tomograms\": len(train_tomos),\n",
    "        \"val_tomograms\": len(val_tomos),\n",
    "        \"train_motors\": train_motors,\n",
    "        \"val_motors\": val_motors,\n",
    "        \"train_slices\": train_slices,\n",
    "        \"val_slices\": val_slices\n",
    "    }\n",
    "\n",
    "def add_noize(image, label, noize_level=0.05):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to an image.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.array): Input image.\n",
    "        noize_level (float): Standard deviation of the Gaussian noise.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.array: Noisy image.\n",
    "    \"\"\"\n",
    "    # Validation data\n",
    "    if not isinstance(image, np.ndarray):\n",
    "        raise ValueError(\"Image must be a numpy array.\")\n",
    "    if image.ndim != 3:\n",
    "        raise ValueError(\"Image must be a 3D array (height, width, channels).\")\n",
    "    if image.shape[2] != 3:\n",
    "        raise ValueError(\"Image must have 3 channels (RGB).\")\n",
    "    if not isinstance(label, list):\n",
    "        raise ValueError(\"Label must be a list.\")\n",
    "    noise = np.random.normal(0, noize_level * 255, image.shape).astype(np.uint8)\n",
    "    noisy_image = cv2.add(image, noise)\n",
    "    return noisy_image, label\n",
    "\n",
    "def add_blur(image, label, blur_level=5):\n",
    "    \"\"\"\n",
    "    Add Gaussian blur to an image.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.array): Input image.\n",
    "        blur_level (int): Size of the Gaussian kernel.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.array: Blurred image.\n",
    "    \"\"\"\n",
    "    # Validation data\n",
    "    if not isinstance(image, np.ndarray):\n",
    "        raise ValueError(\"Image must be a numpy array.\")\n",
    "    if image.ndim != 3:\n",
    "        raise ValueError(\"Image must be a 3D array (height, width, channels).\")\n",
    "    if image.shape[2] != 3:\n",
    "        raise ValueError(\"Image must have 3 channels (RGB).\")\n",
    "    if not isinstance(label, list):\n",
    "        raise ValueError(\"Label must be a list.\")\n",
    "    if blur_level % 2 == 0:\n",
    "        blur_level += 1\n",
    "    blurred_image = cv2.GaussianBlur(image, (blur_level, blur_level), 0)\n",
    "    return blurred_image, label\n",
    "\n",
    "def add_contrast(image, label, contrast_level=1.5):\n",
    "    \"\"\"\n",
    "    Adjust the contrast of an image.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.array): Input image.\n",
    "        contrast_level (float): Contrast adjustment factor.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.array: Contrast-adjusted image.\n",
    "    \"\"\"\n",
    "    # Validation data\n",
    "    if not isinstance(image, np.ndarray):\n",
    "        raise ValueError(\"Image must be a numpy array.\")\n",
    "    if image.shape[2] != 3:\n",
    "        raise ValueError(\"Image must have 3 channels (RGB).\")\n",
    "    if not isinstance(label, list):\n",
    "        raise ValueError(\"Label must be a list.\")\n",
    "    # Validation for contrast level\n",
    "    if contrast_level < 0:\n",
    "        raise ValueError(\"Contrast level must be non-negative.\")\n",
    "    if contrast_level == 0:\n",
    "        return image, label\n",
    "    \n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    l = np.clip(contrast_level * l, 0, 255).astype(np.uint8)\n",
    "    lab = cv2.merge((l, a, b))\n",
    "    contrast_image = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "    return contrast_image, label\n",
    "\n",
    "def add_brightness(image, label, brightness_level=50):\n",
    "    \"\"\"\n",
    "    Adjust the brightness of an image.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.array): Input image.\n",
    "        brightness_level (int): Brightness adjustment value.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.array: Brightness-adjusted image.\n",
    "    \"\"\"\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    v = np.clip(v + brightness_level, 0, 255).astype(np.uint8)\n",
    "    hsv = cv2.merge((h, s, v))\n",
    "    brightness_image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    return brightness_image, label\n",
    "\n",
    "def add_flip(image, label):\n",
    "    \"\"\"\n",
    "    Flip an image horizontally.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.array): Input image.\n",
    "        label (list): List of labels.\n",
    "            line: [x_center, y_center, width, height]\n",
    "    \n",
    "    Returns:\n",
    "        numpy.array: Flipped image.\n",
    "    \"\"\"\n",
    "    flipped_image = cv2.flip(image, 1)\n",
    "    if label is None:\n",
    "        return flipped_image, label\n",
    "\n",
    "    for i, line in enumerate(label):\n",
    "        _, y_center, _, _ = line\n",
    "        y_center = 1 - y_center\n",
    "        line[1] = y_center\n",
    "        label[i] = line\n",
    "    return flipped_image, label\n",
    "\n",
    "def apply_augmentation(augmentations:dict, dir_info=None):\n",
    "    \"\"\"\n",
    "    Apply a series of augmentations to an image.\n",
    "    \n",
    "    Args:\n",
    "        augmentations (dict): List of augmentation functions to apply.\n",
    "        dir_info (dict): Directory information for saving images and labels.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.array: Augmented image.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _process_tomogram_set(tomo_paths, label_paths, images_dir, labels_dir, aug, aug_name): #TODO: add augmentations\n",
    "        \"\"\"\n",
    "        Process a set of tomograms and apply augmentations.\n",
    "        \"\"\"\n",
    "\n",
    "        assert os.path.exists(images_dir), f\"Images directory {images_dir} does not exist.\"\n",
    "        assert os.path.exists(labels_dir), f\"Labels directory {labels_dir} does not exist.\"\n",
    "        assert len(tomo_paths) == len(label_paths), f\"Number of tomograms and labels do not match.\"\n",
    "        \n",
    "        images = []\n",
    "        labels = []\n",
    "        # Placeholder for processing logic\n",
    "        for tomo_path, label_path in tqdm(zip(tomo_paths, label_paths), desc=f\"Processing {aug_name} tomograms\"):\n",
    "            # Load the image and labels\n",
    "            if not os.path.exists(label_path):\n",
    "                print(f\"Warning: {label_path} does not exist, skipping.\")\n",
    "                continue\n",
    "            if not os.path.exists(tomo_path):\n",
    "                print(f\"Warning: {tomo_path} does not exist, skipping.\")\n",
    "                continue\n",
    "            image = Image.open(tomo_path)\n",
    "            image = image.convert(\"RGB\")\n",
    "            image = np.array(image)\n",
    "            with open(label_path, 'r') as f:\n",
    "                label = f.readlines()\n",
    "            \n",
    "            data_name = os.path.basename(tomo_path).replace('.jpg', '')\n",
    "            data_name = f\"{data_name}_{aug_name}.jpg\"\n",
    "            image_dest_path = os.path.join(images_dir, data_name)\n",
    "            label_path = os.path.join(labels_dir, data_name.replace('.jpg', '.txt'))\n",
    "            label_dest_path = os.path.join(labels_dir, data_name.replace('.jpg', '.txt'))\n",
    "            data_name = data_name + f\"_{aug_name}\"\n",
    "            image, label = aug(image, label)\n",
    "            image = Image.fromarray(image)\n",
    "            if image.mode != 'RGB':\n",
    "                image = image.convert('RGB')\n",
    "            image.save(image_dest_path)\n",
    "            images.append(image)\n",
    "            # Save the label\n",
    "            labels.append(label)\n",
    "            with open(label_dest_path, 'w') as f:\n",
    "                for line in label:\n",
    "                    f.write(\" \".join(map(str, line)) + \"\\n\")\n",
    "        return images, labels\n",
    "\n",
    "    # Process training tomograms\n",
    "    train_tomos = os.listdir(dir_info['yolo_images_train'])\n",
    "    train_tomos = [item for item in train_tomos if not item.startswith('.')]\n",
    "    train_tomos = [os.path.join(dir_info['yolo_images_train'], item) for item in train_tomos]\n",
    "    train_labels = os.listdir(dir_info['yolo_labels_train'])\n",
    "    train_labels = [item for item in train_labels if not item.startswith('.')]\n",
    "    train_labels = [os.path.join(dir_info['yolo_labels_train'], item) for item in train_labels]\n",
    "    for aug_name, aug in augmentations.items():\n",
    "        train_slices, train_motors = _process_tomogram_set(train_tomos, train_labels,\n",
    "                                                           dir_info['yolo_images_train'],\n",
    "                                                           dir_info['yolo_labels_train'], aug, aug_name)\n",
    "    \n",
    "    # Process validation tomograms\n",
    "    val_tomos = os.listdir(dir_info['yolo_images_val'])\n",
    "    val_tomos = [item for item in val_tomos if not item.startswith('.')]\n",
    "    val_tomos = [os.path.join(dir_info['yolo_images_val'], item) for item in val_tomos]\n",
    "    val_labels = os.listdir(dir_info['yolo_labels_val'])\n",
    "    val_labels = [item for item in val_labels if not item.startswith('.')]\n",
    "    val_labels = [os.path.join(dir_info['yolo_labels_val'], item) for item in val_labels]\n",
    "    for aug_name, aug in augmentations.items():\n",
    "        val_slices, val_motors = _process_tomogram_set(val_tomos,val_labels,\n",
    "                                                       dir_info['yolo_images_val'],\n",
    "                                                       dir_info['yolo_labels_val'], aug, aug_name)\n",
    "    \n",
    "    # Generate YAML configuration for YOLO training\n",
    "    yaml_content = {\n",
    "        'path': dir_info['yolo_dataset_dir'],\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'names': {0: 'motor'}\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(dir_info['yolo_dataset_dir'], 'dataset.yaml'), 'w') as f:\n",
    "        yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"\\nProcessing Summary:\")\n",
    "    print(f\"- Train set: {len(train_tomos)} tomograms, {len(train_motors)} motors, {len(train_slices)} slices\")\n",
    "    print(f\"- Validation set: {len(val_tomos)} tomograms, {len(val_motors)} motors, {len(val_slices)} slices\")\n",
    "    print(f\"- Total: {len(train_tomos) + len(val_tomos)} tomograms, {len(train_motors) + len(val_motors)} motors, {len(train_slices) + len(val_slices)} slices\")\n",
    "    \n",
    "    return {\n",
    "        \"dataset_dir\": dir_info['yolo_dataset_dir'],\n",
    "        \"yaml_path\": os.path.join(dir_info['yolo_dataset_dir'], 'dataset.yaml'),\n",
    "        \"train_tomograms\": len(train_tomos),\n",
    "        \"val_tomograms\": len(val_tomos),\n",
    "        \"train_motors\": train_motors,\n",
    "        \"val_motors\": val_motors,\n",
    "        \"train_slices\": train_slices,\n",
    "        \"val_slices\": val_slices\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2806e933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motors in the dataset: 831\n",
      "Found 362 unique tomograms with motors\n",
      "Split: 289 tomograms for training, 73 tomograms for validation\n",
      "Will process approximately 3905 slices for training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d168d37a305c4f8ba78f263a9aad0011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing training motors:   0%|          | 0/355 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will process approximately 1056 slices for validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147666431a8341228fbf57fbb264f1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing validation motors:   0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Summary:\n",
      "- Train set: 289 tomograms, 355 motors, 2595 slices\n",
      "- Validation set: 73 tomograms, 96 motors, 702 slices\n",
      "- Total: 362 tomograms, 451 motors, 3297 slices\n"
     ]
    }
   ],
   "source": [
    "yolo_dataset_dir = os.path.join(\"../input\",\"yolo_dataset\")\n",
    "os.makedirs(yolo_dataset_dir, exist_ok=True)\n",
    "yolo_images_train = os.path.join(yolo_dataset_dir, \"images/train\")\n",
    "yolo_images_val = os.path.join(yolo_dataset_dir, \"images/val\")\n",
    "yolo_labels_train = os.path.join(yolo_dataset_dir, \"labels/train\")\n",
    "yolo_labels_val = os.path.join(yolo_dataset_dir, \"labels/val\")\n",
    "os.makedirs(yolo_images_train, exist_ok=True)\n",
    "os.makedirs(yolo_images_val, exist_ok=True)\n",
    "os.makedirs(yolo_labels_train, exist_ok=True)\n",
    "os.makedirs(yolo_labels_val, exist_ok=True)\n",
    "\n",
    "dir_info = {\n",
    "    \"data_path\": \"../input/full_data\",\n",
    "    \"train_dir\": \"../input/full_data/train\",\n",
    "    # \"data_path\": \"../input/data\",\n",
    "    # \"train_dir\": \"../input/data/train\",\n",
    "    \"yolo_dataset_dir\": yolo_dataset_dir,\n",
    "    \"yolo_images_train\": yolo_images_train,\n",
    "    \"yolo_labels_train\": yolo_labels_train,\n",
    "    \"yolo_images_val\": yolo_images_val,\n",
    "    \"yolo_labels_val\": yolo_labels_val\n",
    "}\n",
    "\n",
    "# Prepare the YOLO dataset\n",
    "summary = prepare_yolo_dataset(trust=TRUST, train_split=TRAIN_SPLIT, dir_info=dir_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de7ebd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb769ce8d10b4798994dff0753c9dae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing blur tomograms: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae113b80a16142e18c13d8ed1309f8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing flip tomograms: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m augmentations = {\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# \"contrast\": add_contrast,\u001b[39;00m\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# \"brightness\": add_brightness,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# \"noize\": add_noize\u001b[39;00m\n\u001b[32m      7\u001b[39m }\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m summary = \u001b[43mapply_augmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugmentations\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugmentations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdir_info\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 324\u001b[39m, in \u001b[36mapply_augmentation\u001b[39m\u001b[34m(augmentations, dir_info)\u001b[39m\n\u001b[32m    322\u001b[39m train_labels = [os.path.join(dir_info[\u001b[33m'\u001b[39m\u001b[33myolo_labels_train\u001b[39m\u001b[33m'\u001b[39m], item) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m train_labels]\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m aug_name, aug \u001b[38;5;129;01min\u001b[39;00m augmentations.items():\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     train_slices, train_motors = \u001b[43m_process_tomogram_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_tomos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m                                                       \u001b[49m\u001b[43mdir_info\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43myolo_images_train\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m                                                       \u001b[49m\u001b[43mdir_info\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43myolo_labels_train\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[38;5;66;03m# Process validation tomograms\u001b[39;00m\n\u001b[32m    329\u001b[39m val_tomos = os.listdir(dir_info[\u001b[33m'\u001b[39m\u001b[33myolo_images_val\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 303\u001b[39m, in \u001b[36mapply_augmentation.<locals>._process_tomogram_set\u001b[39m\u001b[34m(tomo_paths, label_paths, images_dir, labels_dir, aug, aug_name)\u001b[39m\n\u001b[32m    301\u001b[39m label_dest_path = os.path.join(labels_dir, data_name.replace(\u001b[33m'\u001b[39m\u001b[33m.jpg\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m.txt\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m    302\u001b[39m data_name = data_name + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maug_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m image, label = \u001b[43maug\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m image = Image.fromarray(image)\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m image.mode != \u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 253\u001b[39m, in \u001b[36madd_flip\u001b[39m\u001b[34m(image, label)\u001b[39m\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m flipped_image, label\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(label):\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     _, y_center, _, _ = line\n\u001b[32m    254\u001b[39m     y_center = \u001b[32m1\u001b[39m - y_center\n\u001b[32m    255\u001b[39m     line[\u001b[32m1\u001b[39m] = y_center\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "augmentations = {\n",
    "    # \"contrast\": add_contrast,\n",
    "    # \"brightness\": add_brightness,\n",
    "    \"blur\": add_blur,\n",
    "    \"flip\": add_flip,\n",
    "    # \"noize\": add_noize\n",
    "}\n",
    "\n",
    "summary = apply_augmentation(augmentations=augmentations, dir_info=dir_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
